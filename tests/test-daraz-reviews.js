/**
 * Test script for Daraz review scraping
 * Tests the review extraction with pagination
 */

require('dotenv').config();
const mongoose = require('mongoose');
const DarazScraper = require('../src/scrapers/daraz/daraz-scraper');
const { logger } = require('../src/utils/logger');

// Test product URL with manageable number of reviews (3-5 pages)
const TEST_URL = 'https://www.daraz.pk/products/55-4k-tv-k85-4k-i433450646-s2080827295.html';

async function testReviewScraping() {
  let scraper = null;

  try {
    logger.info('ğŸ§ª Starting Daraz review scraping test...\n');

    // Connect to database
    logger.info('ğŸ“¦ Connecting to database...');
    await mongoose.connect(process.env.MONGODB_URI);
    logger.info('âœ… Database connected\n');

    // Initialize scraper
    scraper = new DarazScraper();
    await scraper.initialize();

    // First scrape the product to ensure we're on the page
    logger.info('\nğŸ“¦ Scraping product first...');
    const productData = await scraper.scrapeProduct(TEST_URL);

    if (productData) {
      logger.info(`\nâœ… Product scraped: ${productData.name}`);
      logger.info(`   Rating: ${productData.average_rating}/5`);
      logger.info(`   Review count: ${productData.review_count}`);
    } // Now scrape ALL reviews with pagination
    const expectedReviews = productData.review_count || 50;
    const maxPagesNeeded = Math.ceil(expectedReviews / 5) + 2; // 5 reviews per page + buffer

    logger.info(
      `\nğŸ“ Scraping ALL reviews (expecting ~${expectedReviews}, max ${maxPagesNeeded} pages)...`
    );
    const reviews = await scraper.scrapeReviews(TEST_URL, {
      maxPages: maxPagesNeeded,
      maxReviews: expectedReviews + 10, // Buffer for any extra
      scrollToReviews: true,
    }); // Save to database
    logger.info('\nğŸ’¾ Saving product and reviews to database...');

    // Save product first
    const savedProduct = await scraper.saveProduct(productData);
    logger.info(`   âœ… Product saved with ID: ${savedProduct._id}`);

    // Save reviews (returns {saved, skipped})
    const saveResult = await scraper.saveReviews(savedProduct._id, reviews);
    logger.info(
      `   âœ… ${saveResult.saved} new reviews saved, ${saveResult.skipped} duplicates skipped`
    );

    // Display results
    logger.info('\n========================================');
    logger.info('ğŸ“Š REVIEW SCRAPING RESULTS');
    logger.info('========================================');
    logger.info(`Total reviews scraped: ${reviews.length}`);
    logger.info(`New reviews saved: ${saveResult.saved}`);
    logger.info(`Duplicates skipped: ${saveResult.skipped}\n`);

    // Display each review
    reviews.forEach((review, index) => {
      logger.info(`\n--- Review ${index + 1} ---`);
      logger.info(`â­ Rating: ${review.rating}/5`);
      logger.info(`ğŸ‘¤ Author: ${review.reviewer_name}`);
      logger.info(
        `ğŸ“… Date: ${review.platform_metadata?.original_date_text || 'N/A'} (${review.review_date.toLocaleDateString()})`
      );
      logger.info(`âœ… Verified: ${review.verified_purchase ? 'Yes' : 'No'}`);
      logger.info(
        `ğŸ“ Content: ${review.text.substring(0, 100)}${review.text.length > 100 ? '...' : ''}`
      );
      logger.info(`ğŸ–¼ï¸  Images: ${review.images.length}`);
      logger.info(`ğŸ·ï¸  Variant: ${review.platform_metadata?.variant_purchased || 'N/A'}`);
      logger.info(`ğŸ‘ Helpful: ${review.helpful_votes}`);
      if (review.platform_metadata?.seller_reply) {
        logger.info(
          `ğŸ’¬ Seller replied: ${review.platform_metadata.seller_reply.content.substring(0, 50)}...`
        );
      }
    });

    // Summary statistics
    logger.info('\n========================================');
    logger.info('ğŸ“ˆ SUMMARY STATISTICS');
    logger.info('========================================');

    const avgRating = reviews.reduce((sum, r) => sum + r.rating, 0) / reviews.length || 0;
    const verifiedCount = reviews.filter(r => r.verified_purchase).length;
    const withImages = reviews.filter(r => r.images.length > 0).length;
    const totalLikes = reviews.reduce((sum, r) => sum + r.helpful_votes, 0);

    logger.info(`Average Rating: ${avgRating.toFixed(1)}/5`);
    logger.info(
      `Verified Reviews: ${verifiedCount}/${reviews.length} (${((verifiedCount / reviews.length) * 100).toFixed(0)}%)`
    );
    logger.info(`Reviews with Images: ${withImages}/${reviews.length}`);
    logger.info(`Total Helpful Votes: ${totalLikes}`);

    // Rating distribution
    const ratingDist = { 1: 0, 2: 0, 3: 0, 4: 0, 5: 0 };
    reviews.forEach(r => {
      if (r.rating >= 1 && r.rating <= 5) {
        ratingDist[r.rating]++;
      }
    });
    logger.info('\nRating Distribution:');
    for (let i = 5; i >= 1; i--) {
      const count = ratingDist[i];
      const bar = 'â–ˆ'.repeat(count) + 'â–‘'.repeat(Math.max(0, 10 - count));
      logger.info(`  ${i}â­: ${bar} ${count}`);
    }

    logger.info('\nâœ… Review scraping test completed successfully!');
  } catch (error) {
    logger.error('âŒ Test failed:', error);
    console.error(error);
  } finally {
    // Cleanup
    if (scraper) {
      await scraper.close();
    }
    await mongoose.disconnect();
    logger.info('\nğŸ”’ Cleanup complete');
  }
}

// Run the test
testReviewScraping();
